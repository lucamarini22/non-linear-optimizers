# non-linear-optimizers
Non linear optimizers frequently used in the optimization of the loss in neural networks, implemented from scratch using only Python and numpy.
- Gradient descent
- Momentum
- Nesterov momentum
- AdaGrad
- RMSProp
- Adam
